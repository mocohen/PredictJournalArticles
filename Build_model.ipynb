{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict relevant Journal Articles\n",
    "\n",
    "Included in the database is a list of journal articles from a number of journals, some of which are relevant to the Voth Group research areas, and most that are not relevant. Here we will use SVMs to predict the relevant articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from time import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "import logging\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data from database\n",
    "\n",
    "First, create connection with database. Then, pull out all journal article titles.\n",
    "\n",
    "Note: Some of the journal articles are not actually \"journal articles\", so I have just remoeved them from the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file = 'test.db'\n",
    "db_conn = sqlite3.connect(db_file)\n",
    "c = db_conn.cursor()\n",
    "\n",
    "\n",
    "article_titles = []\n",
    "article_isRelevant = []\n",
    "articles = []\n",
    "results = c.execute('SELECT title, isVoth FROM Articles WHERE created_at < ?', ('2018-06-01 00:00:00',))\n",
    "for title, isVoth in results:\n",
    "    if 'Spotlights' not in title and 'Editorial' not in title and 'News at a glance' not in title:\n",
    "        article_titles.append(title)\n",
    "        article_isRelevant.append(isVoth)\n",
    "        articles.append((title, isVoth))\n",
    "\n",
    "\n",
    "data = pd.DataFrame.from_records(articles, columns=['title', 'isvoth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into test and training data sets. This is not actually necessary here, since I have added Cross Validation for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(\n",
    "    article_titles, article_isRelevant, test_size=0.25, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting the different classes\n",
    "In this case, we should be more willing to accept false positives rather than false negatives. I.e. there are likely more relevant articles that the Voth group should be reading, but have not read. Additionally, while looking through the literature, tt is more preferable to look at some extra articles that are not relevant, versus potentially missing out on an important journal article. Therefore, instead of weighting based on inverse frequency, I have increased the weight for relevant articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight_is_relevant = 1\n",
    "weight_is_relevant = 2*1/(np.sum(article_isRelevant)/ len(article_isRelevant))\n",
    "weight_is_not_relevant = 1\n",
    "weights = {0:weight_is_not_relevant, 1:weight_is_relevant}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model\n",
    "I built the model in a number of steps and therefore used `sklearn.pipeline.Pipeline` to run through the different steps.\n",
    "* First, I used the bag-of-words approach to build the parameter set. The `CountVectorizer` function will build this matrix.\n",
    "* Since we want to pick out the unique, important words in a journal title, I used the term-frequency times inverse document-frequency transformer\n",
    "* Last, I used the stochastic gradient descent classifier to build a linear SVM.\n",
    "\n",
    "Additionally, I used the `GridSearchCV` to identify the ideal parameters for the `SGDClassifier` using 3-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': array([1.e-10, 1.e-09, 1.e-08, 1.e-07, 1.e-06]),\n",
      " 'clf__max_iter': (50, 100, 1000, 2000),\n",
      " 'clf__penalty': ('l2', 'elasticnet')}\n",
      "Fitting 3 folds for each of 40 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=6)]: Done  38 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=6)]: Done 120 out of 120 | elapsed: 17.1min finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', random_state=42,\n",
    "                                           max_iter=1000, tol=None, class_weight=weights)),])\n",
    "\n",
    "parameters = {\n",
    "    #'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    #'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': np.logspace(-10, -6, num=5),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__max_iter': (50,100,1000,2000),\n",
    "    #'clf__n_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(clf, parameters, n_jobs=6, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in clf.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(article_titles, article_isRelevant)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = grid_search.best_estimator_.predict(test_X)\n",
    "\n",
    "print(metrics.classification_report(test_y, predicted,\n",
    "    target_names=['is not relevant', 'is relevant']))\n",
    "cnf_matrix = metrics.confusion_matrix(test_y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the false positives to see if they make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#title')\n",
    "num = 0\n",
    "for i, val in enumerate(predicted):\n",
    "    if val == 1 and test_y[i] != val and num < 15:\n",
    "        print( test_X[i])\n",
    "        num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can see that many of the papers are about molecular dynaimcs and coarse-grained simulations, as well as about proteins and membranes, all things that the Voth Group is interested in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = (cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Below we can see how often our model correctly predicts whether an article is relevant or not. This model overestimates the articles that are relevant for the Voth Group, which is exactly what we wanted. Here, only 30 articles are mislabeled. Using inverse frequency for weights in the model reduces the error even further - only a handful of articles are mislabeled. But, as I noted above, we expect that the set of relevant articles is actually larger than the set from the database, so we should prefer to have a reasonable number of articles that are labeled as not relevant, but are actually relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['is not relevant', 'is relevant'],\n",
    "                      title='Confusion matrix, without normalization')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['is not relevant', 'is relevant'], normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "plt.show('normalized_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model persistence\n",
    "\n",
    "Save the model using joblib so that we can access it at some later point in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(grid_search, 'model.pkl') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
